{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b113f7f-5aa7-4e19-afbf-acfa9bc513d3",
   "metadata": {},
   "source": [
    "# Positive samples for the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a68fbd0-0574-4eff-aaac-394669257a1b",
   "metadata": {},
   "source": [
    "## Extracting relevant information from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd83140e-6297-458f-8993-7e1ec5d0e267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pydicom\n",
    "import png\n",
    "import math\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_path = '/home/szelesteya/projects/EMBED_Open_Data/'\n",
    "tables_path = data_path + 'tables/'\n",
    "image_root_path = '/media/szelesteya/F824D4D024D492CC/EMBED-images/'\n",
    "image_dcm_path = image_root_path + 'dicom-positive/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa5f2086-6f8c-4473-895a-f7a59a7b0048",
   "metadata": {},
   "outputs": [],
   "source": [
    "cli_df = pd.read_csv(tables_path + 'EMBED_OpenData_clinical.csv', low_memory=False)\n",
    "\n",
    "# Only keeping result BIRADS-1 and BIRADS-2 screenings\n",
    "pos_cli_df = cli_df[(cli_df.asses.isin(['B','S','M','K'])) &\n",
    "                    ((cli_df.calcfind.notna()) | \n",
    "                     (cli_df.asses.isin(['S','M','K'])))][['Unnamed: 0',\n",
    "                                                           'empi_anon',\n",
    "                                                           'acc_anon',\n",
    "                                                           'side',\n",
    "                                                           'calcfind',\n",
    "                                                           'calcdistri',\n",
    "                                                           'otherfind',\n",
    "                                                           'numfind',\n",
    "                                                           'path_severity',\n",
    "                                                           'age_at_study',\n",
    "                                                           'ETHNICITY_DESC',\n",
    "                                                           'study_date_anon',\n",
    "                                                           'asses']]\n",
    "\n",
    "# Rename columns to prepare for merge\n",
    "pos_cli_df = pos_cli_df.rename(columns={'study_date_anon':'diag_study_date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f1e5fd5-3f26-49b4-ab2f-61a6ef3cccfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading image metadata\n",
    "meta_df = (pd.read_csv(tables_path + 'EMBED_OpenData_metadata_reduced.csv', low_memory=False))\n",
    "meta_red_df = meta_df[  (meta_df['ROI_coords'] != \"()\") &\n",
    "                        (meta_df['FinalImageType'] == '2D') &                         \n",
    "                        (meta_df['spot_mag'] != 1)][[  'empi_anon',\n",
    "                                                                'acc_anon',\n",
    "                                                                'ImageLateralityFinal',\n",
    "                                                                'anon_dicom_path',\n",
    "                                                                'study_date_anon',\n",
    "                                                                'ViewPosition', \n",
    "                                                                'num_roi',\n",
    "                                                                'ROI_coords',\n",
    "                                                                'spot_mag']]\n",
    "\n",
    "# Rename columns to prepare for merge\n",
    "meta_red_ren_df = meta_red_df.rename(columns={'ImageLateralityFinal':'side'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "660f6bc3-602a-48a8-bae4-1d6f9fcd202e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging clinical information with medical ones\n",
    "pos_full_df = pos_cli_df.merge(meta_red_ren_df, on=['empi_anon','acc_anon','side'])\n",
    "\n",
    "# Generate paths for png extraction\n",
    "pos_full_df['relative_dcm_path'] = pos_full_df['anon_dicom_path'].apply(lambda x: '/'.join(x.split('/')[5:]))\n",
    "\n",
    "# Keeping relevant columns\n",
    "pos_empi_df = pos_full_df[['empi_anon',\n",
    "                           'acc_anon',\n",
    "                           'side',\n",
    "                           'asses',\n",
    "                           'age_at_study',\n",
    "                           'calcfind',\n",
    "                           'calcdistri',\n",
    "                           'otherfind',\n",
    "                           'numfind',\n",
    "                           'ViewPosition',\n",
    "                           'num_roi',\n",
    "                           'ROI_coords',\n",
    "                           'ETHNICITY_DESC',\n",
    "                           'study_date_anon',\n",
    "                           'diag_study_date',\n",
    "                           'relative_dcm_path',\n",
    "                           'spot_mag']]\n",
    "\n",
    "# Rename columns to be more consistent\n",
    "pos_empi_df = pos_empi_df.rename(columns={'ETHNICITY_DESC':'eth_desc',\n",
    "                                          'calcfind':'calc_find',\n",
    "                                          'calcdistri':'calc_distrib',\n",
    "                                          'otherfind':'other_find',                                          \n",
    "                                          'ViewPosition':'view_pos',\n",
    "                                          'numfind':'num_find'})\n",
    "\n",
    "\n",
    "# Convert study date so it is easily interpreted by Python\n",
    "pos_empi_df['diag_study_date'] = pd.to_datetime(pos_empi_df['diag_study_date'], errors='coerce', format= '%Y-%m-%d')\n",
    "pos_empi_df['study_date_anon'] = pd.to_datetime(pos_empi_df['study_date_anon'], errors='coerce')\n",
    "\n",
    "# Keeping only screening exams with less than 180 day differential between diagnosis date and last exam date (the diagnosis might be outdated on other circumstances)\n",
    "pos_empi_df['diag_date_diff'] = pos_empi_df.diag_study_date - pos_empi_df.study_date_anon\n",
    "pos_empi_rel_df= pos_empi_df.loc[(pos_empi_df.diag_date_diff.dt.days >= 0) & \n",
    "                                 (pos_empi_df.diag_date_diff.dt.days <= 180)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b99f707e-7cf5-4d05-ad64-882a74d8cdc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>empi_anon</th>\n",
       "      <th>acc_anon</th>\n",
       "      <th>side</th>\n",
       "      <th>asses</th>\n",
       "      <th>age_at_study</th>\n",
       "      <th>calc_find</th>\n",
       "      <th>calc_distrib</th>\n",
       "      <th>other_find</th>\n",
       "      <th>num_find</th>\n",
       "      <th>view_pos</th>\n",
       "      <th>num_roi</th>\n",
       "      <th>ROI_coords</th>\n",
       "      <th>eth_desc</th>\n",
       "      <th>study_date_anon</th>\n",
       "      <th>diag_study_date</th>\n",
       "      <th>relative_dcm_path</th>\n",
       "      <th>spot_mag</th>\n",
       "      <th>diag_date_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38591157</td>\n",
       "      <td>5821249626639895</td>\n",
       "      <td>R</td>\n",
       "      <td>S</td>\n",
       "      <td>61.931456</td>\n",
       "      <td>A,K</td>\n",
       "      <td>G</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>ML</td>\n",
       "      <td>1</td>\n",
       "      <td>((1610, 1045, 1979, 1463),)</td>\n",
       "      <td>Caucasian or White</td>\n",
       "      <td>2015-07-28</td>\n",
       "      <td>2015-07-28</td>\n",
       "      <td>cohort_1/38591157/1.2.846.113979.3.62.1.506522...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81221471</td>\n",
       "      <td>9107681375572026</td>\n",
       "      <td>R</td>\n",
       "      <td>M</td>\n",
       "      <td>51.261833</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>CC</td>\n",
       "      <td>1</td>\n",
       "      <td>((774, 336, 1621, 1062),)</td>\n",
       "      <td>Caucasian or White</td>\n",
       "      <td>2015-03-10</td>\n",
       "      <td>2015-03-10</td>\n",
       "      <td>cohort_1/81221471/1.2.842.113973.3.66.1.510553...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69442705</td>\n",
       "      <td>3307161731730050</td>\n",
       "      <td>L</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>CC</td>\n",
       "      <td>1</td>\n",
       "      <td>((1694, 860, 2298, 1658),)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-09-12</td>\n",
       "      <td>2016-09-12</td>\n",
       "      <td>cohort_1/69442705/1.2.846.113971.3.57.1.541888...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69442705</td>\n",
       "      <td>3307161731730050</td>\n",
       "      <td>L</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>MLO</td>\n",
       "      <td>1</td>\n",
       "      <td>((1708, 1157, 2411, 1768),)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-09-12</td>\n",
       "      <td>2016-09-12</td>\n",
       "      <td>cohort_1/69442705/1.2.846.113971.3.57.1.541888...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69442705</td>\n",
       "      <td>3307161731730050</td>\n",
       "      <td>L</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>CC</td>\n",
       "      <td>1</td>\n",
       "      <td>((1694, 860, 2298, 1658),)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-09-12</td>\n",
       "      <td>2016-09-12</td>\n",
       "      <td>cohort_1/69442705/1.2.846.113971.3.57.1.541888...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>69001424</td>\n",
       "      <td>5173595322425404</td>\n",
       "      <td>L</td>\n",
       "      <td>S</td>\n",
       "      <td>69.844008</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>ML</td>\n",
       "      <td>1</td>\n",
       "      <td>((1354, 1338, 1533, 1545),)</td>\n",
       "      <td>Caucasian or White</td>\n",
       "      <td>2020-06-05</td>\n",
       "      <td>2020-06-05</td>\n",
       "      <td>cohort_2/69001424/1.2.847.113979.3.63.1.609325...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>69001424</td>\n",
       "      <td>5173595322425404</td>\n",
       "      <td>L</td>\n",
       "      <td>S</td>\n",
       "      <td>69.844008</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>ML</td>\n",
       "      <td>1</td>\n",
       "      <td>((1354, 1338, 1533, 1545),)</td>\n",
       "      <td>Caucasian or White</td>\n",
       "      <td>2020-06-05</td>\n",
       "      <td>2020-06-05</td>\n",
       "      <td>cohort_2/69001424/1.2.847.113979.3.63.1.609325...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>90551907</td>\n",
       "      <td>7632750486827326</td>\n",
       "      <td>L</td>\n",
       "      <td>S</td>\n",
       "      <td>45.457470</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>CC</td>\n",
       "      <td>1</td>\n",
       "      <td>((2400, 762, 2722, 1113),)</td>\n",
       "      <td>Asian</td>\n",
       "      <td>2020-05-26</td>\n",
       "      <td>2020-05-26</td>\n",
       "      <td>cohort_2/90551907/1.2.844.113970.3.62.1.612437...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>90551907</td>\n",
       "      <td>7632750486827326</td>\n",
       "      <td>L</td>\n",
       "      <td>S</td>\n",
       "      <td>45.457470</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>MLO</td>\n",
       "      <td>1</td>\n",
       "      <td>((2015, 822, 2257, 1149),)</td>\n",
       "      <td>Asian</td>\n",
       "      <td>2020-05-26</td>\n",
       "      <td>2020-05-26</td>\n",
       "      <td>cohort_2/90551907/1.2.844.113970.3.62.1.612437...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>90551907</td>\n",
       "      <td>7632750486827326</td>\n",
       "      <td>L</td>\n",
       "      <td>S</td>\n",
       "      <td>45.457470</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>ML</td>\n",
       "      <td>1</td>\n",
       "      <td>((2098, 679, 2439, 1101),)</td>\n",
       "      <td>Asian</td>\n",
       "      <td>2020-05-26</td>\n",
       "      <td>2020-05-26</td>\n",
       "      <td>cohort_2/90551907/1.2.844.113970.3.62.1.612437...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>165 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     empi_anon          acc_anon side asses  age_at_study calc_find  \\\n",
       "0     38591157  5821249626639895    R     S     61.931456       A,K   \n",
       "1     81221471  9107681375572026    R     M     51.261833       NaN   \n",
       "2     69442705  3307161731730050    L     M           NaN       NaN   \n",
       "3     69442705  3307161731730050    L     M           NaN       NaN   \n",
       "4     69442705  3307161731730050    L     M           NaN       NaN   \n",
       "..         ...               ...  ...   ...           ...       ...   \n",
       "160   69001424  5173595322425404    L     S     69.844008         A   \n",
       "161   69001424  5173595322425404    L     S     69.844008         A   \n",
       "162   90551907  7632750486827326    L     S     45.457470         A   \n",
       "163   90551907  7632750486827326    L     S     45.457470         A   \n",
       "164   90551907  7632750486827326    L     S     45.457470         A   \n",
       "\n",
       "    calc_distrib other_find  num_find view_pos  num_roi  \\\n",
       "0              G        NaN         1       ML        1   \n",
       "1            NaN        NaN         1       CC        1   \n",
       "2            NaN          N         1       CC        1   \n",
       "3            NaN          N         1      MLO        1   \n",
       "4            NaN          N         1       CC        1   \n",
       "..           ...        ...       ...      ...      ...   \n",
       "160            G        NaN         3       ML        1   \n",
       "161            G        NaN         3       ML        1   \n",
       "162            G        NaN         2       CC        1   \n",
       "163            G        NaN         2      MLO        1   \n",
       "164            G        NaN         2       ML        1   \n",
       "\n",
       "                      ROI_coords            eth_desc study_date_anon  \\\n",
       "0    ((1610, 1045, 1979, 1463),)  Caucasian or White      2015-07-28   \n",
       "1      ((774, 336, 1621, 1062),)  Caucasian or White      2015-03-10   \n",
       "2     ((1694, 860, 2298, 1658),)                 NaN      2016-09-12   \n",
       "3    ((1708, 1157, 2411, 1768),)                 NaN      2016-09-12   \n",
       "4     ((1694, 860, 2298, 1658),)                 NaN      2016-09-12   \n",
       "..                           ...                 ...             ...   \n",
       "160  ((1354, 1338, 1533, 1545),)  Caucasian or White      2020-06-05   \n",
       "161  ((1354, 1338, 1533, 1545),)  Caucasian or White      2020-06-05   \n",
       "162   ((2400, 762, 2722, 1113),)               Asian      2020-05-26   \n",
       "163   ((2015, 822, 2257, 1149),)               Asian      2020-05-26   \n",
       "164   ((2098, 679, 2439, 1101),)               Asian      2020-05-26   \n",
       "\n",
       "    diag_study_date                                  relative_dcm_path  \\\n",
       "0        2015-07-28  cohort_1/38591157/1.2.846.113979.3.62.1.506522...   \n",
       "1        2015-03-10  cohort_1/81221471/1.2.842.113973.3.66.1.510553...   \n",
       "2        2016-09-12  cohort_1/69442705/1.2.846.113971.3.57.1.541888...   \n",
       "3        2016-09-12  cohort_1/69442705/1.2.846.113971.3.57.1.541888...   \n",
       "4        2016-09-12  cohort_1/69442705/1.2.846.113971.3.57.1.541888...   \n",
       "..              ...                                                ...   \n",
       "160      2020-06-05  cohort_2/69001424/1.2.847.113979.3.63.1.609325...   \n",
       "161      2020-06-05  cohort_2/69001424/1.2.847.113979.3.63.1.609325...   \n",
       "162      2020-05-26  cohort_2/90551907/1.2.844.113970.3.62.1.612437...   \n",
       "163      2020-05-26  cohort_2/90551907/1.2.844.113970.3.62.1.612437...   \n",
       "164      2020-05-26  cohort_2/90551907/1.2.844.113970.3.62.1.612437...   \n",
       "\n",
       "     spot_mag diag_date_diff  \n",
       "0         NaN         0 days  \n",
       "1         NaN         0 days  \n",
       "2         NaN         0 days  \n",
       "3         NaN         0 days  \n",
       "4         NaN         0 days  \n",
       "..        ...            ...  \n",
       "160       NaN         0 days  \n",
       "161       NaN         0 days  \n",
       "162       NaN         0 days  \n",
       "163       NaN         0 days  \n",
       "164       NaN         0 days  \n",
       "\n",
       "[165 rows x 18 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_empi_rel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "295b4764-9e52-4ffd-8920-e7256a252614",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path + 'positive_empirical.csv', 'w') as f:\n",
    "    pos_empi_rel_df.to_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7a6357-e741-467c-8eb1-f5c1f3d0a65a",
   "metadata": {},
   "source": [
    "## Pulling the DICOM images with a bash script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dd1b131-3c92-414b-ba54-788feab497b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the path to file to read it in the bash script\n",
    "with open(data_path + 'positive_path.csv', 'w') as f:\n",
    "    pos_empi_rel_df.drop_duplicates(subset=['relative_dcm_path'])['relative_dcm_path'].to_csv(f, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75eb2120-11d5-4f6d-bfe6-764c8e692c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J2 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J3 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J4 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J5 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J6 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J7 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J8 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J9 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J10 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J11 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J12 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J13 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J14 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J15 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J16 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J17 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J18 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J19 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J20 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J21 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J22 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J23 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J24 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J25 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J26 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J27 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J28 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J29 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J30 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J31 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J32 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J33 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J34 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J35 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J36 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J37 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J38 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J39 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J40 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J41 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J42 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J43 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J44 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J45 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J46 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J47 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J48 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J49 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J50 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J51 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J52 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J53 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J54 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J55 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J56 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J57 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J58 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J59 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J60 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J61 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J62 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J63 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J64 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J65 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J66 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J67 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J68 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J69 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J70 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J71 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J72 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J73 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J74 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J75 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J76 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J77 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J78 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J79 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J80 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J81 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J82 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J83 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J84 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J85 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J86 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J87 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J88 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J89 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J90 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J91 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J92 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J93 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J94 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J95 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J96 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J97 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J98 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J99 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J100 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J101 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J102 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J103 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J104 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J105 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J106 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J107 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J108 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J109 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J110 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J111 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J112 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J113 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J114 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J115 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J116 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J117 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J118 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J119 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J120 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J121 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J122 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J123 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J124 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J125 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J126 / 146\n",
      "File already present\n",
      "download: s3://embed-dataset-open/images/cohort_1/21231866/1.2.849.113975.3.61.1.54550052.20170311.1083132/1.2.845.113681.2750851778.1488784065.4233.67640/1.2.826.0.1.3680043.8.498.57576843178887984287123842955719609359.dcm to ../../../../media/szelesteya/F824D4D024D492CC/EMBED-images/dicom-positive/cohort_1/21231866/1.2.849.113975.3.61.1.54550052.20170311.1083132/1.2.845.113681.2750851778.1488784065.4233.67640/1.2.826.0.1.3680043.8.498.57576843178887984287123842955719609359.dcm\n",
      "\u001b[H\u001b[2J14 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J15 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J16 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J17 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J18 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J19 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J20 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J21 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J22 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J23 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J24 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J25 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J26 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J27 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J28 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J29 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J30 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J31 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J32 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J33 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J34 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J35 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J36 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J37 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J38 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J39 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J40 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J41 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J42 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J43 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J44 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J45 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J46 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J47 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J48 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J49 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J50 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J51 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J52 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J53 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J54 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J55 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J56 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J57 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J58 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J59 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J60 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J61 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J62 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J63 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J64 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J65 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J66 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J67 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J68 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J69 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J70 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J71 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J72 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J73 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J74 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J75 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J76 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J77 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J78 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J79 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J80 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J81 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J82 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J83 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J84 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J85 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J86 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J87 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J88 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J89 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J90 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J91 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J92 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J93 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J94 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J95 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J96 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J97 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J98 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J99 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J100 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J101 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J102 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J103 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J104 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J105 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J106 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J107 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J108 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J109 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J110 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J111 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J112 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J113 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J114 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J115 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J116 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J117 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J118 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J119 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J120 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J121 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J122 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J123 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J124 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J125 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J126 / 146\n",
      "File already present\n",
      "\u001b[H\u001b[2J"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$image_dcm_path\" \"{data_path}positive_path.csv\"\n",
    "\n",
    "# Pulling dicom files with AWS CLI (Python API didn't work)\n",
    "dcm_dest_path=\"$1\"\n",
    "dcm_paths=\"$2\"\n",
    "ind=$((1))\n",
    "\n",
    "tail -n +2 $dcm_paths | while IFS= read -r line; do\n",
    "    relative_path=$(echo \"$line\" | awk -v OFS='/' '{$1=$1; print}')\n",
    "    dcm_name=$(echo \"$relative_path\" | cut -d '/' -f 3-)\n",
    "               \n",
    "    file=\"${dcm_dest_path}$relative_path\"\n",
    "    dir=$(dirname $file)\n",
    "    mkdir $dir -p\n",
    "    echo \"$ind / 146\"\n",
    "                    \n",
    "    if [ -f \"$file\" ]; then\n",
    "        echo \"File already present\"\n",
    "    else\n",
    "        if [ -f \"${dcm_dest_path}$dcm_name\" ]; then\n",
    "            dcm=\"${dcm_dest_path}$dcm_name\"\n",
    "            dir_dcm=$(dirname $dcm)\n",
    "            echo \"Moving file from ${dir_dcm}\"\n",
    "            mv \"${dcm}\" \"${dcm_dest_path}$relative_path\"\n",
    "        else\n",
    "            echo \"Pulling file $file\"\n",
    "            aws s3 cp \"s3://embed-dataset-open/images/$relative_path\" \"${file}\" --profile my-dev-profile\n",
    "        fi\n",
    "    fi\n",
    "    \n",
    "\n",
    "    ind=$((ind+1))\n",
    "    clear\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710b52ab-5f03-41c5-a515-c81318cd54b3",
   "metadata": {},
   "source": [
    "### Converting the DICOM images to PNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6257d147-d33b-4a18-afa5-30564bd4e03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get DICOM image metadata\n",
    "class DCM_Tags():\n",
    "    def __init__(self, img_dcm):\n",
    "        try:\n",
    "            self.laterality = img_dcm.ImageLaterality\n",
    "        except AttributeError:\n",
    "            self.laterality = np.nan\n",
    "            \n",
    "        try:\n",
    "            self.view = img_dcm.ViewPosition\n",
    "        except AttributeError:\n",
    "            self.view = np.nan\n",
    "            \n",
    "        try:\n",
    "            self.orientation = img_dcm.PatientOrientation\n",
    "        except AttributeError:\n",
    "            self.orientation = np.nan\n",
    "\n",
    "# Check whether DICOM should be flipped\n",
    "def check_dcm(imgdcm):\n",
    "    # Get DICOM metadata\n",
    "    tags = DCM_Tags(imgdcm)\n",
    "    \n",
    "    # If image orientation tag is defined\n",
    "    if ~pd.isnull(tags.orientation):\n",
    "        # CC view\n",
    "        if tags.view == 'CC':\n",
    "            if tags.orientation[0] == 'P':\n",
    "                flipHorz = True\n",
    "            else:\n",
    "                flipHorz = False\n",
    "            \n",
    "            if (tags.laterality == 'L') & (tags.orientation[1] == 'L'):\n",
    "                flipVert = True\n",
    "            elif (tags.laterality == 'R') & (tags.orientation[1] == 'R'):\n",
    "                flipVert = True\n",
    "            else:\n",
    "                flipVert = False\n",
    "        \n",
    "        # MLO or ML views\n",
    "        elif (tags.view == 'MLO') | (tags.view == 'ML'):\n",
    "            if tags.orientation[0] == 'P':\n",
    "                flipHorz = True\n",
    "            else:\n",
    "                flipHorz = False\n",
    "            \n",
    "            if (tags.laterality == 'L') & ((tags.orientation[1] == 'H') | (tags.orientation[1] == 'HL')):\n",
    "                flipVert = True\n",
    "            elif (tags.laterality == 'R') & ((tags.orientation[1] == 'H') | (tags.orientation[1] == 'HR')):\n",
    "                flipVert = True\n",
    "            else:\n",
    "                flipVert = False\n",
    "        \n",
    "        # Unrecognized view\n",
    "        else:\n",
    "            flipHorz = False\n",
    "            flipVert = False\n",
    "            \n",
    "    # If image orientation tag is undefined\n",
    "    else:\n",
    "        # Flip RCC, RML, and RMLO images\n",
    "        if (tags.laterality == 'R') & ((tags.view == 'CC') | (tags.view == 'ML') | (tags.view == 'MLO')):\n",
    "            flipHorz = True\n",
    "            flipVert = False\n",
    "        else:\n",
    "            flipHorz = False\n",
    "            flipVert = False\n",
    "            \n",
    "    return flipHorz, flipVert\n",
    "\n",
    "# Rescale the intensity of the image to get heterogene images with the bit depth of 14\n",
    "def rescale_to_8bit(image_array):\n",
    "    upper_percentile = np.percentile(image_array.flatten(), 98) # original_max = np.max(image_array)\n",
    "    lower_percentile = np.percentile(image_array.flatten(), 2) # original_min = np.min(image_array)\n",
    "    # max_on_14bit = 16383\n",
    "    max = 255\n",
    "    rescaled_array = (image_array - lower_percentile) / (upper_percentile - lower_percentile)\n",
    "    rescaled_array[rescaled_array < 0] = 0\n",
    "    rescaled_array[rescaled_array > 1] = 1\n",
    "    # rescaled_array = np.round((image_array - original_min) / (original_max - original_min) * max_on_14bit).astype(int)\n",
    "    return np.round(rescaled_array * 255).astype(np.uint8)\n",
    "\n",
    "def generate_png_path(dcm_path):\n",
    "    # Get new file name\n",
    "    split_fn = dcm_path[:-4].split('/')\n",
    "    new_fn = f\"{split_fn[-1]}_conv.png\"\n",
    "    return image_path + new_fn\n",
    "\n",
    "# Save DICOM pixel array as PNG\n",
    "def save_dcm_image_as_png(image, png_filename, bitdepth=8):\n",
    "    with open(png_filename, 'wb') as f:\n",
    "        rescaled = rescale_to_8bit(image)\n",
    "        writer = png.Writer(height=rescaled.shape[0], \n",
    "                            width=rescaled.shape[1], \n",
    "                            bitdepth=bitdepth, \n",
    "                            greyscale=True)\n",
    "        writer.write(f, rescaled.tolist())\n",
    "\n",
    "def generate_png_path(acc_anon, png_dir):\n",
    "    # Get new file name\n",
    "    new_fn = f\"{acc_anon}_neg_conv.png\"\n",
    "    return f'{png_dir}/{new_fn}'\n",
    "\n",
    "# Convert list of DICOMs to PNGs\n",
    "def process_dcm_list(dcm_list, png_list):    \n",
    "    for i, dcm_path in enumerate(dcm_list):    \n",
    "        if not Path(png_list[i]).exists():\n",
    "            print(f\"Processing DICOM #{i}...\")\n",
    "            \n",
    "            # Load DICOM\n",
    "            dcm = pydicom.dcmread(dcm_path)\n",
    "            img = dcm.pixel_array\n",
    "            \n",
    "            # Check if a horizontal flip is necessary\n",
    "            horz, _ = check_dcm(dcm)\n",
    "            if horz:\n",
    "                # Flip img horizontally\n",
    "                img = np.fliplr(img)\n",
    "            \n",
    "            # Save PNG            \n",
    "            save_dcm_image_as_png(img, png_list[i])\n",
    "\n",
    "def extract_images(data_file_name, dcm_dir, png_dir):\n",
    "    # Provide a list of DICOM paths and a target directory\n",
    "    dcm_list = []\n",
    "    df = pd.read_csv(data_file_name)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        path = dcm_dir  + row['relative_dcm_path']\n",
    "        if Path(path).exists():\n",
    "            dcm_list.append(path)\n",
    "        \n",
    "    # Insert png path\n",
    "    df.loc[:,'png_path'] = df['acc_anon'].apply(lambda x: generate_png_path(x, png_dir))\n",
    "\n",
    "    # Convert DICOMs\n",
    "    process_dcm_list(dcm_list, df['png_path'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d74abbb4-a36e-42ae-b8be-eda7c94f2846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DICOM #0...\n",
      "Processing DICOM #1...\n",
      "Processing DICOM #2...\n",
      "Processing DICOM #6...\n",
      "Processing DICOM #8...\n",
      "Processing DICOM #9...\n",
      "Processing DICOM #10...\n",
      "Processing DICOM #11...\n",
      "Processing DICOM #13...\n",
      "Processing DICOM #17...\n",
      "Processing DICOM #19...\n",
      "Processing DICOM #20...\n",
      "Processing DICOM #21...\n",
      "Processing DICOM #23...\n",
      "Processing DICOM #25...\n",
      "Processing DICOM #28...\n",
      "Processing DICOM #32...\n",
      "Processing DICOM #34...\n",
      "Processing DICOM #35...\n",
      "Processing DICOM #37...\n",
      "Processing DICOM #38...\n",
      "Processing DICOM #42...\n",
      "Processing DICOM #43...\n",
      "Processing DICOM #46...\n",
      "Processing DICOM #48...\n",
      "Processing DICOM #50...\n",
      "Processing DICOM #51...\n",
      "Processing DICOM #52...\n",
      "Processing DICOM #53...\n",
      "Processing DICOM #55...\n",
      "Processing DICOM #56...\n",
      "Processing DICOM #57...\n",
      "Processing DICOM #59...\n",
      "Processing DICOM #61...\n",
      "Processing DICOM #63...\n",
      "Processing DICOM #65...\n",
      "Processing DICOM #66...\n",
      "Processing DICOM #68...\n",
      "Processing DICOM #70...\n",
      "Processing DICOM #71...\n",
      "Processing DICOM #72...\n",
      "Processing DICOM #73...\n",
      "Processing DICOM #77...\n",
      "Processing DICOM #79...\n",
      "Processing DICOM #82...\n",
      "Processing DICOM #83...\n",
      "Processing DICOM #84...\n",
      "Processing DICOM #85...\n",
      "Processing DICOM #87...\n",
      "Processing DICOM #88...\n",
      "Processing DICOM #90...\n",
      "Processing DICOM #91...\n",
      "Processing DICOM #92...\n",
      "Processing DICOM #94...\n",
      "Processing DICOM #96...\n",
      "Processing DICOM #97...\n",
      "Processing DICOM #98...\n",
      "Processing DICOM #99...\n",
      "Processing DICOM #101...\n",
      "Processing DICOM #102...\n",
      "Processing DICOM #108...\n",
      "Processing DICOM #109...\n",
      "Processing DICOM #110...\n",
      "Processing DICOM #111...\n",
      "Processing DICOM #115...\n",
      "Processing DICOM #119...\n",
      "Processing DICOM #123...\n",
      "Processing DICOM #127...\n",
      "Processing DICOM #131...\n",
      "Processing DICOM #132...\n",
      "Processing DICOM #134...\n",
      "Processing DICOM #136...\n",
      "Processing DICOM #138...\n",
      "Processing DICOM #141...\n",
      "Processing DICOM #143...\n",
      "Processing DICOM #144...\n",
      "Processing DICOM #148...\n",
      "Processing DICOM #152...\n",
      "Processing DICOM #158...\n"
     ]
    }
   ],
   "source": [
    "pos_img_emp = extract_images(data_path + 'positive_empirical.csv', image_dcm_path, image_png_path)\n",
    "\n",
    "with open(data_path + 'positive_empirical_png.csv', 'w') as f:\n",
    "    (pos_img_emp).to_csv(f, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260cf8e9-4593-45bd-a14d-8be88dedeb4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
